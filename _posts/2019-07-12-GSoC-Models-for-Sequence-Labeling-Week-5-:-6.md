---
layout: post
title: "GSoC, Sequence Labelling, Week 5 - 6"
date: 2019-07-12
tags: [Google Summer of Code, GSoC, Natural Language Processing, Deep Learning, Sequence Labelling, Flux, Named Entity Recogntion, Part of Speech, NER, POS]
comments: true
categories:
---
Weeks 5 and 6 were mainly about implementing models for sequence labelling.

## Sequence Labelling using Flux

Sequence labelling tasks involves predicting
a sequence of output tags corresponding to each element in the input sequence.
Various tasks in Natural Language Processing,
like Named Entity Recognition, Part of Speech Tagging
fall under this category.

The model implemented was similar to the one proposed by
[(Xuezhe Ma and Eduard Hovy)](https://arxiv.org/pdf/1603.01354.pdf)
for end to end sequence labelling using Bi-LSTM-CNN-CRFs.
It can be broken down to the following three layers-

### Input Embeddings

  The input embeddings consists of Word Embeddings
  and Character level features for each token / word.
  It can also contain additional word features
  like Capitalization or punctuation.

  The GloVe Word Embeddings was loaded using 
  [Embeddings.jl](https://github.com/JuliaText/Embeddings.jl).

```julia
  julia> using Embeddings

  julia> embtable = load_embeddings(GloVe)

  julia> get_word_index = Dict(word => ii for (ii, word) in enumerate(embtable.vocab))

  julia> W_word_Embeddings = embtable.embeddings
```

  The embeddings could be made trainable - `W_word_Embeddings = param(W_word_Embeddings)`

  From this embedding table,
  lookup can be performed by simply slicing the desired column.
  A more convenient way of doing this was using flux onehot vectors for slicing.
  For a one hot encoded word `w_oh` the corresponding lookup value could be obtained simply by `W_word_Embeddings * w_oh`.

  The Character Level representation,
  can be generated from the character vectors using
  CNN with maxpool<sup>[1](#1)</sup> or
  RNN / LSTM <sup>[2](#2)</sup> layers.
  The embeddings lookups were performed similar to Word Embeddings.

Character Representation using CNNs        |  Character Representation using LSTM
:-------------------------:|:-------------------------:
![Character Representation using CNN](../../../images/2019/Char-rep.png)  |  ![Character Representation using LSTM](../../../images/2019/Char-rep-lstm.png){:class="img-responsive"}

### Bi-LSTM layer

  The input embeddings to the bi-directional LSTM were generated by concatenating the character representation with the word embeddings.

### Inference Layer (Conditional Random Fields or Softmax)

  The LSTM gives a sequences of outputs,
  the labels are jointly inferred using a layer of
  Conditional Random Field by performing Viterbi Decode.

  ![Bi-LSTM CRF Layer](../../../images/2019/LSTM-CRF-layer.png)

The code can be found [here](https://github.com/Ayushk4/NER.jl).

### <u> References and Image Credits- </u>

<a name="1"> 1.</a> End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF (Ma et al.) [paper↑](https://arxiv.org/pdf/1603.01354.pdf)

<a name="2"> 2.</a> Neural Architectures for Named Entity Recognition (Lample et al.) [paper↑](https://arxiv.org/pdf/1603.01360.pdf)

<a name="3"> 3.</a>Named Entity Recognition with Bidirectional LSTM-CNNs (Chiu et al.) [paper↑](https://www.aclweb.org/anthology/Q16-1026)

## Other Work Done

- DataDeps.jl [#99](https://github.com/oxinabox/DataDeps.jl/pull/99/): I came across an error in post_fetch_helpers, so sent a PR. The CI test suite was broken so, I moved on to fix that. I went through the tests and learned the usage of `stubs` for testing. I also familiarised myself with the package `ExpectationStubs.jl`

- WordTokenizers.jl [#34](https://github.com/JuliaText/WordTokenizers.jl/pull/34) This PR was about udpating the `josspaper` with details about the `TokenBuffer` API along with various newly added tokenizers for natural language. Being my first experience working on a technical paper, I read about various other similar JOSS papers to understand the approach. I also compared the performance of WordTokenizers.jl with natural language libraries and toolkits offering tokenizers. The results can be found [here](https://github.com/Ayushk4/Tweet_tok_analyse/tree/master/speed).
